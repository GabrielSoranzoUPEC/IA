{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22751,
     "status": "ok",
     "timestamp": 1685085021198,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "_XzCfw6Jdx4q",
    "outputId": "e00e6ed5-374c-498d-cf9d-ece32312b4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 09:41:34.977480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 09:41:35.370618: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-26 09:41:35.370649: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-26 09:41:36.628348: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-26 09:41:36.628630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-26 09:41:36.628641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-26 09:41:37.708352: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-26 09:41:37.708626: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-26 09:41:37.708648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DELLIUTGS): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "!python make_args.py --model_size bert-large-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1685085026716,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "EnfYvHcelULc"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('data/args.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "args=pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtM5B0J-YWJ0"
   },
   "source": [
    "On charge le modèle pré-entraîné:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111085,
     "status": "ok",
     "timestamp": 1685085226142,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "1Fd5-V_7Z-BQ",
    "outputId": "9c4a3703-d258-4bb6-b8e8-e9c45c5e28a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 10:44:32.078241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 10:44:32.506200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:44:32.506248: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-09 10:44:33.694530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:44:33.694853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:44:33.694861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-09 10:44:34.922622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:44:34.922912: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-09 10:44:34.922941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DELLIUTGS): /proc/driver/nvidia/version does not exist\n",
      "06/09/2023 10:44:38 AM [INFO]: Loading tokenizer and model...\n",
      "06/09/2023 10:44:38 AM [INFO]: TensorFlow version 2.11.0 available.\n",
      "06/09/2023 10:44:38 AM [INFO]: PyTorch version 2.0.0+cpu available.\n",
      "06/09/2023 10:44:39 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/gs/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "06/09/2023 10:44:39 AM [INFO]: Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/09/2023 10:44:39 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/gs/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config:  {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2023 10:44:47 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n",
      "06/09/2023 10:44:52 AM [INFO]: Loaded checkpoint model.\n",
      "06/09/2023 10:44:53 AM [INFO]: Loaded model and optimizer.\n",
      "06/09/2023 10:44:53 AM [INFO]: Done!\n"
     ]
    }
   ],
   "source": [
    "from src.tasks.infer import infer_from_trained\n",
    "\n",
    "inferer = infer_from_trained(args=None,detect_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7i2h9miYWJ1"
   },
   "source": [
    "On recherche la relation entre les deux items marqués:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3767,
     "status": "ok",
     "timestamp": 1685085235128,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "Eay_DymQcYBY",
    "outputId": "a2807494-d155-418a-8e9b-5c6a3c919f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The surprise [E1]visit[/E1] caused a [E2]frenzy[/E2] on the already chaotic trading floor.\n",
      "Predicted:  Cause-Effect(e1,e2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"The surprise [E1]visit[/E1] caused a [E2]frenzy[/E2] on the already chaotic trading floor.\"\n",
    "inferer.infer_one_sentence(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFCbjrsuYWJ2"
   },
   "source": [
    "On lui demande de détecter lui-même les items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1685085237715,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "qQAGGBDPYWJ3",
    "outputId": "88a46130-0ea4-4a68-ce48-91210b2508c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  [E1]The surprise visit[/E1] caused [E2]a frenzy on the already chaotic trading floor[/E2] .\n",
      "Predicted:  Cause-Effect(e1,e2) \n",
      "\n",
      "Sentence:  [E2]The surprise visit[/E2] caused [E1]a frenzy on the already chaotic trading floor[/E1] .\n",
      "Predicted:  Cause-Effect(e2,e1) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"The surprise visit caused a frenzy on the already chaotic trading floor.\"\n",
    "inferer.infer_sentence(test, detect_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDFof4KPYWJ5"
   },
   "source": [
    "Problème de non détection des items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ltzrTofuYWJ5",
    "outputId": "1953d702-9f83-4bed-ea1c-c90fe5e7d288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found less than 2 entities!\n"
     ]
    }
   ],
   "source": [
    "test2=\"Take the cube and put it in the black box.\"\n",
    "inferer.infer_sentence(test2, detect_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_XlHs0uYWJ7"
   },
   "source": [
    "Le problème vient de la bibliothèque Spacy que ne détecte pas bien certaines entités:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT8Rb-tWYWJ_"
   },
   "source": [
    "Par contre la méthode suivante semble plus simple et semble donner tous les groupes nominaux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2cN34mYYWKD"
   },
   "source": [
    "Il convient donc maintenant de fabriquer unes fonction pour fabriquer la liste des phrases annotées `[E1]...[/E1]` et `[E2]...[/E2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7373,
     "status": "ok",
     "timestamp": 1685085257726,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "P-RrvgopYWKD"
   },
   "outputs": [],
   "source": [
    "# Bibliothèques nécessaires\n",
    "from itertools import combinations,permutations\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1685085276709,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "jiGN8zf7YWKF",
    "outputId": "9b24843d-4912-44dc-c0c4-f4cb6eaf6d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E2]the cube[/E2] is [E1]the box[/E1].\n"
     ]
    }
   ],
   "source": [
    "def add_markers(sentence, noun_chunk1, noun_chunk2):\n",
    "    # Analyser la phrase en utilisant Spacy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Trouver le début et la fin du noun_chunk dans la phrase\n",
    "    start1 = noun_chunk1.start_char\n",
    "    end1 = noun_chunk1.end_char\n",
    "    start2 = noun_chunk2.start_char\n",
    "    end2 = noun_chunk2.end_char\n",
    "    \n",
    "    if start1 < start2:\n",
    "        # Ajouter les marqueurs \"[C]\" et \"[/C]\"\n",
    "        modified_sentence = sentence[:start1] + \"[E1]\" + sentence[start1:end1] + \"[/E1]\" + sentence[end1:start2] + \"[E2]\" + sentence[start2:end2] + \"[/E2]\" + sentence[end2:]\n",
    "    else:\n",
    "        modified_sentence = sentence[:start2] + \"[E2]\" + sentence[start2:end2] + \"[/E2]\" + sentence[end2:start1] + \"[E1]\" + sentence[start1:end1] + \"[/E1]\" + sentence[end1:]\n",
    "\n",
    "    \n",
    "    return modified_sentence\n",
    "\n",
    "# Exemple d'utilisation\n",
    "sentence = \"the cube is the box.\"\n",
    "doc = nlp(sentence)\n",
    "nc = list(doc.noun_chunks)\n",
    "modified_sentence = add_markers(sentence, nc[1], nc[0])\n",
    "print(modified_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1685085280417,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "Kh1RrAH_YWKF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In [E1]the beginning[/E1] [E2]God[/E2] created the heaven and the earth.',\n",
       " 'In [E1]the beginning[/E1] God created [E2]the heaven[/E2] and the earth.',\n",
       " 'In [E1]the beginning[/E1] God created the heaven and [E2]the earth[/E2].',\n",
       " 'In [E2]the beginning[/E2] [E1]God[/E1] created the heaven and the earth.',\n",
       " 'In the beginning [E1]God[/E1] created [E2]the heaven[/E2] and the earth.',\n",
       " 'In the beginning [E1]God[/E1] created the heaven and [E2]the earth[/E2].',\n",
       " 'In [E2]the beginning[/E2] God created [E1]the heaven[/E1] and the earth.',\n",
       " 'In the beginning [E2]God[/E2] created [E1]the heaven[/E1] and the earth.',\n",
       " 'In the beginning God created [E1]the heaven[/E1] and [E2]the earth[/E2].',\n",
       " 'In [E2]the beginning[/E2] God created the heaven and [E1]the earth[/E1].',\n",
       " 'In the beginning [E2]God[/E2] created the heaven and [E1]the earth[/E1].',\n",
       " 'In the beginning God created [E2]the heaven[/E2] and [E1]the earth[/E1].']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_annotated(sent):\n",
    "    doc=nlp(sent)\n",
    "    nps=doc.noun_chunks\n",
    "    sentences=[]\n",
    "    for np1, np2 in permutations([np for np in nps], 2):\n",
    "        sentences.append(add_markers(sent,np1,np2))\n",
    "    return sentences\n",
    "\n",
    "get_annotated(\"In the beginning God created the heaven and the earth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5L7X42rYWKF"
   },
   "source": [
    "La méthode n'est pas parfaite car le modèle de recherche des relations se trompe, comme on le voit sur le lien entre les cubes et les sphères ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1685085287591,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "TN6jJorcYWKF",
    "outputId": "e1cf697b-8818-4475-e997-88461daaa22a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  [E1]You[/E1] must put [E2]the cubes[/E2] and the spheres in the boxe\n",
      "Predicted:  Instrument-Agency(e2,e1) \n",
      "\n",
      "2\n",
      "Sentence:  [E1]You[/E1] must put the cubes and [E2]the spheres[/E2] in the boxe\n",
      "Predicted:  Instrument-Agency(e2,e1) \n",
      "\n",
      "2\n",
      "Sentence:  [E1]You[/E1] must put the cubes and the spheres in [E2]the boxe[/E2]\n",
      "Predicted:  Instrument-Agency(e2,e1) \n",
      "\n",
      "2\n",
      "Sentence:  [E2]You[/E2] must put [E1]the cubes[/E1] and the spheres in the boxe\n",
      "Predicted:  Other \n",
      "\n",
      "1\n",
      "Sentence:  You must put [E1]the cubes[/E1] and [E2]the spheres[/E2] in the boxe\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n",
      "5\n",
      "Sentence:  You must put [E1]the cubes[/E1] and the spheres in [E2]the boxe[/E2]\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n",
      "5\n",
      "Sentence:  [E2]You[/E2] must put the cubes and [E1]the spheres[/E1] in the boxe\n",
      "Predicted:  Other \n",
      "\n",
      "1\n",
      "Sentence:  You must put [E2]the cubes[/E2] and [E1]the spheres[/E1] in the boxe\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n",
      "5\n",
      "Sentence:  You must put the cubes and [E1]the spheres[/E1] in [E2]the boxe[/E2]\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n",
      "5\n",
      "Sentence:  [E2]You[/E2] must put the cubes and the spheres in [E1]the boxe[/E1]\n",
      "Predicted:  Other \n",
      "\n",
      "1\n",
      "Sentence:  You must put [E2]the cubes[/E2] and the spheres in [E1]the boxe[/E1]\n",
      "Predicted:  Other \n",
      "\n",
      "1\n",
      "Sentence:  You must put the cubes and [E2]the spheres[/E2] in [E1]the boxe[/E1]\n",
      "Predicted:  Other \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test5=\"You must put the cubes and the spheres in the boxe\"\n",
    "sentences=get_annotated(test5)\n",
    "for sentence in sentences:\n",
    "    print(inferer.infer_one_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kY8DWQVYWKG"
   },
   "source": [
    "Idem ci-dessous ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1685085319181,
     "user": {
      "displayName": "Gabriel Soranzo",
      "userId": "02669135601935751302"
     },
     "user_tz": -120
    },
    "id": "fFOYA6A2YWKG",
    "outputId": "2946bb1b-6bf5-4433-8a84-f6e504a5f390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  [E1]Each boxes[/E1] must be filled with [E2]one cube[/E2] and one sphere.\n",
      "Predicted:  Content-Container(e2,e1) \n",
      "\n",
      "16\n",
      "Sentence:  [E1]Each boxes[/E1] must be filled with one cube and [E2]one sphere[/E2].\n",
      "Predicted:  Content-Container(e2,e1) \n",
      "\n",
      "16\n",
      "Sentence:  [E2]Each boxes[/E2] must be filled with [E1]one cube[/E1] and one sphere.\n",
      "Predicted:  Content-Container(e1,e2) \n",
      "\n",
      "6\n",
      "Sentence:  Each boxes must be filled with [E1]one cube[/E1] and [E2]one sphere[/E2].\n",
      "Predicted:  Content-Container(e1,e2) \n",
      "\n",
      "6\n",
      "Sentence:  [E2]Each boxes[/E2] must be filled with one cube and [E1]one sphere[/E1].\n",
      "Predicted:  Content-Container(e1,e2) \n",
      "\n",
      "6\n",
      "Sentence:  Each boxes must be filled with [E2]one cube[/E2] and [E1]one sphere[/E1].\n",
      "Predicted:  Content-Container(e2,e1) \n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "test5=\"Each boxes must be filled with one cube and one sphere.\"\n",
    "sentences=get_annotated(test5)\n",
    "for sentence in sentences:\n",
    "    print(inferer.infer_one_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llIjlPNuYWKG"
   },
   "source": [
    "Le modèle doit être améliorée:\n",
    "- utiliser la méthode explicitée dans l'article en lien avec le Github pour un pré-entraînement non supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle est cependant bon pour trouver distinguer conteneur et contenu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rE2X5UK6YWKH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  [E1]the box[/E1] must be filled with [E2]the cubes[/E2]\n",
      "Predicted:  Content-Container(e2,e1) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test='[E1]the box[/E1] must be filled with [E2]the cubes[/E2]'\n",
    "inferer.infer_one_sentence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  You must put [E1]the cubes[/E1] in [E2]the black box[/E2].\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test='You must put [E1]the cubes[/E1] in [E2]the black box[/E2].'\n",
    "inferer.infer_one_sentence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  You must put [E2]the cubes[/E2] in [E1]the black box[/E1].\n",
      "Predicted:  Other \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test='You must put [E2]the cubes[/E2] in [E1]the black box[/E1].'\n",
    "inferer.infer_one_sentence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You must put [E1]the cubes[/E1] in [E2]the black box[/E2].'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_markers(sentence, noun_chunk1, noun_chunk2):\n",
    "    # Analyser la phrase en utilisant Spacy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Trouver le début et la fin du noun_chunk dans la phrase\n",
    "    start1 = sentence.find(noun_chunk1)\n",
    "    end1 = start1+len(noun_chunk1)\n",
    "    start2 = sentence.find(noun_chunk2)\n",
    "    end2 = start2+len(noun_chunk2)\n",
    "    \n",
    "    if start1 < start2:\n",
    "        # Ajouter les marqueurs \"[C]\" et \"[/C]\"\n",
    "        modified_sentence = sentence[:start1] + \"[E1]\" + sentence[start1:end1] + \"[/E1]\" + sentence[end1:start2] + \"[E2]\" + sentence[start2:end2] + \"[/E2]\" + sentence[end2:]\n",
    "    else:\n",
    "        modified_sentence = sentence[:start2] + \"[E2]\" + sentence[start2:end2] + \"[/E2]\" + sentence[end2:start1] + \"[E1]\" + sentence[start1:end1] + \"[/E1]\" + sentence[end1:]\n",
    "\n",
    "    \n",
    "    return modified_sentence\n",
    "\n",
    "test='You must put the cubes in the black box.'\n",
    "add_markers(test,\"the cubes\",\"the black box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findContainer(sent,item1,item2):\n",
    "    sent_marked=add_markers(sent,item1,item2)\n",
    "    rel=inferer.infer_one_sentence(sent_marked)\n",
    "    if rel==6 or rel==5:\n",
    "        return 2\n",
    "    elif rel==18 or rel==16:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  You must put [E1]the cubes[/E1] in [E2]the black box[/E2]\n",
      "Predicted:  Entity-Destination(e1,e2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findContainer(\"You must put the cubes in the black box\",\"the cubes\",\"the black box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
