{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "On récupère le code:"
      ],
      "metadata": {
        "id": "MUxrC9B7XKDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IglCo5xtGE9x",
        "outputId": "e1a4c914-790c-4361-d373-15d78b322555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-Relation-Extraction'...\n",
            "remote: Enumerating objects: 350, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 350 (delta 133), reused 120 (delta 120), pack-reused 203\u001b[K\n",
            "Receiving objects: 100% (350/350), 579.01 KiB | 1.25 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/plkmo/BERT-Relation-Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va dans le dossier:"
      ],
      "metadata": {
        "id": "xVdmAR7ZXQS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lScDGSaI9H9h",
        "outputId": "a4dfa5ee-7887-44fd-efd5-e3bc070ce307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-Relation-Extraction\n"
          ]
        }
      ],
      "source": [
        "%cd BERT-Relation-Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On récupère des bibliothèques essentielles (redémarrer à ce niveau):"
      ],
      "metadata": {
        "id": "WN2h0YnaZDJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spnss2e0J5Ep",
        "outputId": "b19b1c10-623f-4bdd-bee7-316f0cbcaa58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.8.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.40.0)\n"
          ]
        }
      ],
      "source": [
        "pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penser à retourner dans le dossier:"
      ],
      "metadata": {
        "id": "XdoK-bJlpHpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd BERT-Relation-Extraction\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbsQmegfpF6k",
        "outputId": "68bb3a9c-bc7e-4d0d-bf8f-bdcc02b38884"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'BERT-Relation-Extraction'\n",
            "/content/BERT-Relation-Extraction\n",
            "/content/BERT-Relation-Extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On récupère d'autre bibliothèques:"
      ],
      "metadata": {
        "id": "GFYcFWxLZcnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5DlMCE-J5oF",
        "outputId": "ccf58205-0d7e-48d7-bcb4-9e1d7af371e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encore d'autres:"
      ],
      "metadata": {
        "id": "MJyaw5SAZllP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb0-10PmKAh_",
        "outputId": "54d000a7-0ebe-4602-fe1f-d2f764c0d24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-23 09:22:35.358549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encore d'autres:"
      ],
      "metadata": {
        "id": "aUVLesj4ZsCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Ry10WcLMQj",
        "outputId": "cf88b5fc-21d0-4898-8edb-d1d6817723c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérifions que c'est ok:"
      ],
      "metadata": {
        "id": "YE8V82sXZvrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oFfhetqrKmWA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parfois besoin de faire cela (why?):"
      ],
      "metadata": {
        "id": "1aYoVtD_ZzbQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BZs6mMe0RpE0"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On récupère les données d'entraînement:"
      ],
      "metadata": {
        "id": "6rA0kMJNZ4hx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA7XJx-IsTMO",
        "outputId": "570dbc0e-55fd-43ab-efc7-eb7d3aaaffa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Relation-Classification' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sahitya0000/Relation-Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZjlnULetUvE"
      },
      "source": [
        "On unzip le fichier `Relation-Classification/corpus/SemEval....zip` dans `BERT-Relation-Extraction/data/`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./Relation-Classification/corpus/SemEval2010_task8_all_data.zip -d data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRLXXABjaD2k",
        "outputId": "851b8562-03eb-4ccc-a905-45fdf58c6a5d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./Relation-Classification/corpus/SemEval2010_task8_all_data.zip\n",
            "   creating: data/SemEval2010_task8_all_data/\n",
            "   creating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/\n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/answer_key1.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/answer_key2.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/answer_key3.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/answer_key5.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/proposed_answer1.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/proposed_answer2.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/proposed_answer3.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/proposed_answer4.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/proposed_answer5.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/README.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/result_scores1.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/result_scores2.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/result_scores3.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/result_scores5.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/semeval2010_task8_format_checker.pl  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl  \n",
            "   creating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing/\n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing/README.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing/TEST_FILE.txt  \n",
            "   creating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/\n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT  \n",
            "   creating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/\n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/README.txt  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Guidelines.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation1.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation2.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation3.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation4.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation5.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation6.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation7.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation8.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/Task8_Relation9.pdf  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_DISTRIB.TXT  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT  \n",
            "  inflating: data/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_TEST_DISTRIB.TXT  \n",
            "  inflating: data/SemEval2010_task8_all_data/SEMEVAL_TASK8_FULL_RELEASE_README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut le faire dans un terminal avec la commande suivante pour ouvrir le terminal sous Colab: <br/>\n",
        "`!pip install colab-xterm` <br/>\n",
        "`%load_ext colabxterm` <br/>\n",
        "`%xterm`"
      ],
      "metadata": {
        "id": "p8OAmD96aib3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quelques autre bibliothèques à importer:"
      ],
      "metadata": {
        "id": "ZWvv4n0fas02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLp7obeVuA-z",
        "outputId": "96f378ed-89cf-4288-8bb0-8a00d42ecd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.138-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Collecting botocore<1.30.0,>=1.29.138 (from boto3)\n",
            "  Downloading botocore-1.29.138-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.138->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.138->boto3) (1.26.15)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.138->boto3) (1.16.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=54af99011a42b09f30055b83b23c069cf003d1be61f79d3ef6bcacd885b1f270\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: jmespath, botocore, seqeval, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.138 botocore-1.29.138 jmespath-1.0.1 s3transfer-0.6.1 seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On lance l'apprentissage:"
      ],
      "metadata": {
        "id": "we5TKAobpck5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx4HHD9Xm_8k",
        "outputId": "15480cea-9881-406e-931a-96dd8fd0948a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-23 09:25:25.388592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/23/2023 09:25:31 AM [INFO]: TensorFlow version 2.12.0 available.\n",
            "05/23/2023 09:25:31 AM [INFO]: PyTorch version 2.0.1+cu118 available.\n",
            "05/23/2023 09:25:31 AM [INFO]: Pre-trained blanks tokenizer not found, initializing new tokenizer...\n",
            "05/23/2023 09:25:32 AM [INFO]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/23/2023 09:25:32 AM [INFO]: Adding [E1] to the vocabulary\n",
            "05/23/2023 09:25:32 AM [INFO]: Adding [/E1] to the vocabulary\n",
            "05/23/2023 09:25:32 AM [INFO]: Adding [E2] to the vocabulary\n",
            "05/23/2023 09:25:32 AM [INFO]: Adding [/E2] to the vocabulary\n",
            "05/23/2023 09:25:32 AM [INFO]: Adding [BLANK] to the vocabulary\n",
            "05/23/2023 09:25:32 AM [INFO]: Saved BERT tokenizer at ./data/BERT_tokenizer.pkl\n",
            "05/23/2023 09:25:32 AM [INFO]: Reading training file ./data/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT...\n",
            "05/23/2023 09:25:32 AM [INFO]: Reading test file ./data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT...\n",
            "05/23/2023 09:25:32 AM [INFO]: Mapping relations to IDs...\n",
            "100% 8000/8000 [00:00<00:00, 2354366.55it/s]\n",
            "prog-bar: 100% 2717/2717 [00:00<00:00, 169247.23it/s]\n",
            "prog-bar: 100% 8000/8000 [00:00<00:00, 160055.87it/s]\n",
            "05/23/2023 09:25:32 AM [INFO]: Finished and saved!\n",
            "05/23/2023 09:25:32 AM [INFO]: Tokenizing data...\n",
            "prog-bar: 100% 8000/8000 [00:04<00:00, 1682.65it/s]\n",
            "prog-bar: 100% 8000/8000 [00:00<00:00, 59801.59it/s]\n",
            "\n",
            "Invalid rows/total: 0/8000\n",
            "05/23/2023 09:25:37 AM [INFO]: Tokenizing data...\n",
            "prog-bar: 100% 2717/2717 [00:02<00:00, 1144.86it/s]\n",
            "prog-bar: 100% 2717/2717 [00:00<00:00, 106124.10it/s]\n",
            "\n",
            "Invalid rows/total: 0/2717\n",
            "05/23/2023 09:25:39 AM [INFO]: Loaded 8000 Training samples.\n",
            "05/23/2023 09:25:40 AM [INFO]: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp2pgidyc9\n",
            "100% 433/433 [00:00<00:00, 402333.55B/s]\n",
            "05/23/2023 09:25:41 AM [INFO]: copying /tmp/tmp2pgidyc9 to cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:25:41 AM [INFO]: creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:25:41 AM [INFO]: removing temp file /tmp/tmp2pgidyc9\n",
            "05/23/2023 09:25:41 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:25:41 AM [INFO]: Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:25:42 AM [INFO]: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpg84mlcl_\n",
            "100% 440473133/440473133 [00:35<00:00, 12410620.64B/s]\n",
            "05/23/2023 09:26:18 AM [INFO]: copying /tmp/tmpg84mlcl_ to cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "05/23/2023 09:26:19 AM [INFO]: creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "05/23/2023 09:26:19 AM [INFO]: removing temp file /tmp/tmpg84mlcl_\n",
            "05/23/2023 09:26:19 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Model config:  {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:26:22 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n",
            "05/23/2023 09:26:22 AM [INFO]: FREEZING MOST HIDDEN LAYERS...\n",
            "[FROZE]: embeddings.word_embeddings.weight\n",
            "[FROZE]: embeddings.position_embeddings.weight\n",
            "[FROZE]: embeddings.token_type_embeddings.weight\n",
            "[FROZE]: embeddings.LayerNorm.weight\n",
            "[FROZE]: embeddings.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.0.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.0.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.0.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.0.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.0.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.0.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.0.output.dense.weight\n",
            "[FROZE]: encoder.layer.0.output.dense.bias\n",
            "[FROZE]: encoder.layer.0.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.0.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.1.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.1.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.1.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.1.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.1.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.1.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.1.output.dense.weight\n",
            "[FROZE]: encoder.layer.1.output.dense.bias\n",
            "[FROZE]: encoder.layer.1.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.1.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.2.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.2.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.2.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.2.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.2.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.2.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.2.output.dense.weight\n",
            "[FROZE]: encoder.layer.2.output.dense.bias\n",
            "[FROZE]: encoder.layer.2.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.2.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.3.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.3.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.3.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.3.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.3.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.3.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.3.output.dense.weight\n",
            "[FROZE]: encoder.layer.3.output.dense.bias\n",
            "[FROZE]: encoder.layer.3.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.3.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.4.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.4.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.4.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.4.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.4.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.4.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.4.output.dense.weight\n",
            "[FROZE]: encoder.layer.4.output.dense.bias\n",
            "[FROZE]: encoder.layer.4.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.4.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.5.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.5.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.5.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.5.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.5.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.5.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.5.output.dense.weight\n",
            "[FROZE]: encoder.layer.5.output.dense.bias\n",
            "[FROZE]: encoder.layer.5.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.5.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.6.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.6.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.6.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.6.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.6.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.6.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.6.output.dense.weight\n",
            "[FROZE]: encoder.layer.6.output.dense.bias\n",
            "[FROZE]: encoder.layer.6.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.6.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.7.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.7.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.7.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.7.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.7.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.7.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.7.output.dense.weight\n",
            "[FROZE]: encoder.layer.7.output.dense.bias\n",
            "[FROZE]: encoder.layer.7.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.7.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.8.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.8.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.8.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.8.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.8.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.8.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.8.output.dense.weight\n",
            "[FROZE]: encoder.layer.8.output.dense.bias\n",
            "[FROZE]: encoder.layer.8.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.8.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.9.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.9.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.9.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.9.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.9.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.9.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.9.output.dense.weight\n",
            "[FROZE]: encoder.layer.9.output.dense.bias\n",
            "[FROZE]: encoder.layer.9.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.9.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.10.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.10.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.10.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.10.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.10.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.10.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.10.output.dense.weight\n",
            "[FROZE]: encoder.layer.10.output.dense.bias\n",
            "[FROZE]: encoder.layer.10.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.10.output.LayerNorm.bias\n",
            "[FREE]: encoder.layer.11.attention.self.query.weight\n",
            "[FREE]: encoder.layer.11.attention.self.query.bias\n",
            "[FREE]: encoder.layer.11.attention.self.key.weight\n",
            "[FREE]: encoder.layer.11.attention.self.key.bias\n",
            "[FREE]: encoder.layer.11.attention.self.value.weight\n",
            "[FREE]: encoder.layer.11.attention.self.value.bias\n",
            "[FREE]: encoder.layer.11.attention.output.dense.weight\n",
            "[FREE]: encoder.layer.11.attention.output.dense.bias\n",
            "[FREE]: encoder.layer.11.attention.output.LayerNorm.weight\n",
            "[FREE]: encoder.layer.11.attention.output.LayerNorm.bias\n",
            "[FREE]: encoder.layer.11.intermediate.dense.weight\n",
            "[FREE]: encoder.layer.11.intermediate.dense.bias\n",
            "[FREE]: encoder.layer.11.output.dense.weight\n",
            "[FREE]: encoder.layer.11.output.dense.bias\n",
            "[FREE]: encoder.layer.11.output.LayerNorm.weight\n",
            "[FREE]: encoder.layer.11.output.LayerNorm.bias\n",
            "[FREE]: pooler.dense.weight\n",
            "[FREE]: pooler.dense.bias\n",
            "[FREE]: classification_layer.weight\n",
            "[FREE]: classification_layer.bias\n",
            "05/23/2023 09:26:22 AM [INFO]: Starting training process...\n",
            "[Epoch: 1,   800/ 8000 points] total loss, accuracy per batch: 2.795, 0.136\n",
            "[Epoch: 1,  1600/ 8000 points] total loss, accuracy per batch: 2.634, 0.165\n",
            "[Epoch: 1,  2400/ 8000 points] total loss, accuracy per batch: 2.546, 0.206\n",
            "[Epoch: 1,  3200/ 8000 points] total loss, accuracy per batch: 2.403, 0.282\n",
            "[Epoch: 1,  4000/ 8000 points] total loss, accuracy per batch: 2.197, 0.364\n",
            "[Epoch: 1,  4800/ 8000 points] total loss, accuracy per batch: 2.051, 0.352\n",
            "[Epoch: 1,  5600/ 8000 points] total loss, accuracy per batch: 1.886, 0.407\n",
            "[Epoch: 1,  6400/ 8000 points] total loss, accuracy per batch: 1.760, 0.453\n",
            "[Epoch: 1,  7200/ 8000 points] total loss, accuracy per batch: 1.589, 0.531\n",
            "[Epoch: 1,  8000/ 8000 points] total loss, accuracy per batch: 1.424, 0.562\n",
            "05/23/2023 09:26:57 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  8.91it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 9 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 14 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 10 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 8 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 12 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 11 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 2 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 3 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 5 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 7 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 13 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 6 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 4 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 16 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 17 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 15 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 18 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "05/23/2023 09:27:06 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:27:06 AM [INFO]:   accuracy = 0.6221222109533469\n",
            "05/23/2023 09:27:06 AM [INFO]:   f1 = 0.4693599636858829\n",
            "05/23/2023 09:27:06 AM [INFO]:   precision = 0.5391032325338895\n",
            "05/23/2023 09:27:06 AM [INFO]:   recall = 0.41559485530546625\n",
            "Epoch finished, took 44.10 seconds.\n",
            "Losses at Epoch 1: 2.1284395\n",
            "Train accuracy at Epoch 1: 0.3460000\n",
            "Test f1 at Epoch 1: 0.4693600\n",
            "[Epoch: 2,   800/ 8000 points] total loss, accuracy per batch: 1.339, 0.621\n",
            "[Epoch: 2,  1600/ 8000 points] total loss, accuracy per batch: 1.216, 0.616\n",
            "[Epoch: 2,  2400/ 8000 points] total loss, accuracy per batch: 1.203, 0.634\n",
            "[Epoch: 2,  3200/ 8000 points] total loss, accuracy per batch: 1.149, 0.636\n",
            "[Epoch: 2,  4000/ 8000 points] total loss, accuracy per batch: 1.048, 0.654\n",
            "[Epoch: 2,  4800/ 8000 points] total loss, accuracy per batch: 1.013, 0.694\n",
            "[Epoch: 2,  5600/ 8000 points] total loss, accuracy per batch: 1.000, 0.682\n",
            "[Epoch: 2,  6400/ 8000 points] total loss, accuracy per batch: 0.931, 0.718\n",
            "[Epoch: 2,  7200/ 8000 points] total loss, accuracy per batch: 0.952, 0.713\n",
            "[Epoch: 2,  8000/ 8000 points] total loss, accuracy per batch: 0.877, 0.729\n",
            "05/23/2023 09:27:44 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:10<00:00,  8.35it/s]\n",
            "05/23/2023 09:27:54 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:27:54 AM [INFO]:   accuracy = 0.7390720081135903\n",
            "05/23/2023 09:27:54 AM [INFO]:   f1 = 0.6579984239558707\n",
            "05/23/2023 09:27:54 AM [INFO]:   precision = 0.6442901234567902\n",
            "05/23/2023 09:27:54 AM [INFO]:   recall = 0.6723027375201288\n",
            "Epoch finished, took 44.80 seconds.\n",
            "Losses at Epoch 2: 1.0727214\n",
            "Train accuracy at Epoch 2: 0.6696250\n",
            "Test f1 at Epoch 2: 0.6579984\n",
            "[Epoch: 3,   800/ 8000 points] total loss, accuracy per batch: 0.860, 0.741\n",
            "[Epoch: 3,  1600/ 8000 points] total loss, accuracy per batch: 0.806, 0.731\n",
            "[Epoch: 3,  2400/ 8000 points] total loss, accuracy per batch: 0.823, 0.728\n",
            "[Epoch: 3,  3200/ 8000 points] total loss, accuracy per batch: 0.819, 0.743\n",
            "[Epoch: 3,  4000/ 8000 points] total loss, accuracy per batch: 0.767, 0.767\n",
            "[Epoch: 3,  4800/ 8000 points] total loss, accuracy per batch: 0.807, 0.733\n",
            "[Epoch: 3,  5600/ 8000 points] total loss, accuracy per batch: 0.798, 0.748\n",
            "[Epoch: 3,  6400/ 8000 points] total loss, accuracy per batch: 0.815, 0.743\n",
            "[Epoch: 3,  7200/ 8000 points] total loss, accuracy per batch: 0.794, 0.738\n",
            "[Epoch: 3,  8000/ 8000 points] total loss, accuracy per batch: 0.770, 0.746\n",
            "05/23/2023 09:28:33 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  8.61it/s]\n",
            "05/23/2023 09:28:43 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:28:43 AM [INFO]:   accuracy = 0.7702459432048682\n",
            "05/23/2023 09:28:43 AM [INFO]:   f1 = 0.6985645933014355\n",
            "05/23/2023 09:28:43 AM [INFO]:   precision = 0.7093117408906883\n",
            "05/23/2023 09:28:43 AM [INFO]:   recall = 0.6881382560879812\n",
            "Epoch finished, took 44.02 seconds.\n",
            "Losses at Epoch 3: 0.8058260\n",
            "Train accuracy at Epoch 3: 0.7416250\n",
            "Test f1 at Epoch 3: 0.6985646\n",
            "[Epoch: 4,   800/ 8000 points] total loss, accuracy per batch: 0.655, 0.792\n",
            "[Epoch: 4,  1600/ 8000 points] total loss, accuracy per batch: 0.703, 0.777\n",
            "[Epoch: 4,  2400/ 8000 points] total loss, accuracy per batch: 0.727, 0.774\n",
            "[Epoch: 4,  3200/ 8000 points] total loss, accuracy per batch: 0.721, 0.751\n",
            "[Epoch: 4,  4000/ 8000 points] total loss, accuracy per batch: 0.701, 0.785\n",
            "[Epoch: 4,  4800/ 8000 points] total loss, accuracy per batch: 0.719, 0.765\n",
            "[Epoch: 4,  5600/ 8000 points] total loss, accuracy per batch: 0.717, 0.761\n",
            "[Epoch: 4,  6400/ 8000 points] total loss, accuracy per batch: 0.641, 0.796\n",
            "[Epoch: 4,  7200/ 8000 points] total loss, accuracy per batch: 0.645, 0.789\n",
            "[Epoch: 4,  8000/ 8000 points] total loss, accuracy per batch: 0.698, 0.769\n",
            "05/23/2023 09:29:24 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  8.90it/s]\n",
            "05/23/2023 09:29:34 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:29:34 AM [INFO]:   accuracy = 0.7772692697768762\n",
            "05/23/2023 09:29:34 AM [INFO]:   f1 = 0.7115085536547435\n",
            "05/23/2023 09:29:34 AM [INFO]:   precision = 0.6963470319634704\n",
            "05/23/2023 09:29:34 AM [INFO]:   recall = 0.7273449920508744\n",
            "Epoch finished, took 42.89 seconds.\n",
            "Losses at Epoch 4: 0.6927701\n",
            "Train accuracy at Epoch 4: 0.7760000\n",
            "Test f1 at Epoch 4: 0.7115086\n",
            "[Epoch: 5,   800/ 8000 points] total loss, accuracy per batch: 0.618, 0.809\n",
            "[Epoch: 5,  1600/ 8000 points] total loss, accuracy per batch: 0.579, 0.815\n",
            "[Epoch: 5,  2400/ 8000 points] total loss, accuracy per batch: 0.640, 0.804\n",
            "[Epoch: 5,  3200/ 8000 points] total loss, accuracy per batch: 0.642, 0.791\n",
            "[Epoch: 5,  4000/ 8000 points] total loss, accuracy per batch: 0.552, 0.825\n",
            "[Epoch: 5,  4800/ 8000 points] total loss, accuracy per batch: 0.618, 0.807\n",
            "[Epoch: 5,  5600/ 8000 points] total loss, accuracy per batch: 0.610, 0.800\n",
            "[Epoch: 5,  6400/ 8000 points] total loss, accuracy per batch: 0.640, 0.801\n",
            "[Epoch: 5,  7200/ 8000 points] total loss, accuracy per batch: 0.629, 0.794\n",
            "[Epoch: 5,  8000/ 8000 points] total loss, accuracy per batch: 0.582, 0.821\n",
            "05/23/2023 09:30:22 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.19it/s]\n",
            "05/23/2023 09:30:32 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:30:32 AM [INFO]:   accuracy = 0.7920131845841786\n",
            "05/23/2023 09:30:32 AM [INFO]:   f1 = 0.7161366313309776\n",
            "05/23/2023 09:30:32 AM [INFO]:   precision = 0.7075252133436772\n",
            "05/23/2023 09:30:32 AM [INFO]:   recall = 0.724960254372019\n",
            "Epoch finished, took 41.59 seconds.\n",
            "Losses at Epoch 5: 0.6106501\n",
            "Train accuracy at Epoch 5: 0.8067500\n",
            "Test f1 at Epoch 5: 0.7161366\n",
            "[Epoch: 6,   800/ 8000 points] total loss, accuracy per batch: 0.550, 0.824\n",
            "[Epoch: 6,  1600/ 8000 points] total loss, accuracy per batch: 0.539, 0.830\n",
            "[Epoch: 6,  2400/ 8000 points] total loss, accuracy per batch: 0.566, 0.825\n",
            "[Epoch: 6,  3200/ 8000 points] total loss, accuracy per batch: 0.569, 0.818\n",
            "[Epoch: 6,  4000/ 8000 points] total loss, accuracy per batch: 0.581, 0.816\n",
            "[Epoch: 6,  4800/ 8000 points] total loss, accuracy per batch: 0.563, 0.816\n",
            "[Epoch: 6,  5600/ 8000 points] total loss, accuracy per batch: 0.551, 0.821\n",
            "[Epoch: 6,  6400/ 8000 points] total loss, accuracy per batch: 0.549, 0.812\n",
            "[Epoch: 6,  7200/ 8000 points] total loss, accuracy per batch: 0.568, 0.814\n",
            "[Epoch: 6,  8000/ 8000 points] total loss, accuracy per batch: 0.582, 0.802\n",
            "05/23/2023 09:31:31 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.06it/s]\n",
            "05/23/2023 09:31:41 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:31:41 AM [INFO]:   accuracy = 0.7883747464503043\n",
            "05/23/2023 09:31:41 AM [INFO]:   f1 = 0.7161066048667439\n",
            "05/23/2023 09:31:41 AM [INFO]:   precision = 0.6985681989449887\n",
            "05/23/2023 09:31:41 AM [INFO]:   recall = 0.7345483359746434\n",
            "Epoch finished, took 41.60 seconds.\n",
            "Losses at Epoch 6: 0.5617593\n",
            "Train accuracy at Epoch 6: 0.8178750\n",
            "Test f1 at Epoch 6: 0.7161066\n",
            "[Epoch: 7,   800/ 8000 points] total loss, accuracy per batch: 0.502, 0.844\n",
            "[Epoch: 7,  1600/ 8000 points] total loss, accuracy per batch: 0.582, 0.810\n",
            "[Epoch: 7,  2400/ 8000 points] total loss, accuracy per batch: 0.550, 0.814\n",
            "[Epoch: 7,  3200/ 8000 points] total loss, accuracy per batch: 0.559, 0.811\n",
            "[Epoch: 7,  4000/ 8000 points] total loss, accuracy per batch: 0.488, 0.841\n",
            "[Epoch: 7,  4800/ 8000 points] total loss, accuracy per batch: 0.514, 0.834\n",
            "[Epoch: 7,  5600/ 8000 points] total loss, accuracy per batch: 0.475, 0.849\n",
            "[Epoch: 7,  6400/ 8000 points] total loss, accuracy per batch: 0.511, 0.826\n",
            "[Epoch: 7,  7200/ 8000 points] total loss, accuracy per batch: 0.494, 0.841\n",
            "[Epoch: 7,  8000/ 8000 points] total loss, accuracy per batch: 0.481, 0.851\n",
            "05/23/2023 09:32:37 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.18it/s]\n",
            "05/23/2023 09:32:46 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:32:46 AM [INFO]:   accuracy = 0.7932302231237323\n",
            "05/23/2023 09:32:46 AM [INFO]:   f1 = 0.7201270345375148\n",
            "05/23/2023 09:32:46 AM [INFO]:   precision = 0.7164296998420221\n",
            "05/23/2023 09:32:46 AM [INFO]:   recall = 0.7238627294493216\n",
            "Epoch finished, took 41.12 seconds.\n",
            "Losses at Epoch 7: 0.5155033\n",
            "Train accuracy at Epoch 7: 0.8321250\n",
            "Test f1 at Epoch 7: 0.7201270\n",
            "[Epoch: 8,   800/ 8000 points] total loss, accuracy per batch: 0.469, 0.856\n",
            "[Epoch: 8,  1600/ 8000 points] total loss, accuracy per batch: 0.476, 0.840\n",
            "[Epoch: 8,  2400/ 8000 points] total loss, accuracy per batch: 0.441, 0.863\n",
            "[Epoch: 8,  3200/ 8000 points] total loss, accuracy per batch: 0.429, 0.873\n",
            "[Epoch: 8,  4000/ 8000 points] total loss, accuracy per batch: 0.500, 0.825\n",
            "[Epoch: 8,  4800/ 8000 points] total loss, accuracy per batch: 0.500, 0.858\n",
            "[Epoch: 8,  5600/ 8000 points] total loss, accuracy per batch: 0.468, 0.841\n",
            "[Epoch: 8,  6400/ 8000 points] total loss, accuracy per batch: 0.499, 0.835\n",
            "[Epoch: 8,  7200/ 8000 points] total loss, accuracy per batch: 0.500, 0.844\n",
            "[Epoch: 8,  8000/ 8000 points] total loss, accuracy per batch: 0.537, 0.819\n",
            "05/23/2023 09:33:46 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.30it/s]\n",
            "05/23/2023 09:33:55 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:33:55 AM [INFO]:   accuracy = 0.7931921906693712\n",
            "05/23/2023 09:33:55 AM [INFO]:   f1 = 0.7237415774871185\n",
            "05/23/2023 09:33:55 AM [INFO]:   precision = 0.7304\n",
            "05/23/2023 09:33:55 AM [INFO]:   recall = 0.7172034564021995\n",
            "Epoch finished, took 41.08 seconds.\n",
            "Losses at Epoch 8: 0.4818632\n",
            "Train accuracy at Epoch 8: 0.8452500\n",
            "Test f1 at Epoch 8: 0.7237416\n",
            "[Epoch: 9,   800/ 8000 points] total loss, accuracy per batch: 0.458, 0.855\n",
            "[Epoch: 9,  1600/ 8000 points] total loss, accuracy per batch: 0.451, 0.864\n",
            "[Epoch: 9,  2400/ 8000 points] total loss, accuracy per batch: 0.442, 0.853\n",
            "[Epoch: 9,  3200/ 8000 points] total loss, accuracy per batch: 0.463, 0.855\n",
            "[Epoch: 9,  4000/ 8000 points] total loss, accuracy per batch: 0.445, 0.861\n",
            "[Epoch: 9,  4800/ 8000 points] total loss, accuracy per batch: 0.470, 0.856\n",
            "[Epoch: 9,  5600/ 8000 points] total loss, accuracy per batch: 0.458, 0.865\n",
            "[Epoch: 9,  6400/ 8000 points] total loss, accuracy per batch: 0.441, 0.865\n",
            "[Epoch: 9,  7200/ 8000 points] total loss, accuracy per batch: 0.446, 0.868\n",
            "[Epoch: 9,  8000/ 8000 points] total loss, accuracy per batch: 0.395, 0.865\n",
            "05/23/2023 09:34:44 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.27it/s]\n",
            "05/23/2023 09:34:53 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:34:53 AM [INFO]:   accuracy = 0.7939655172413793\n",
            "05/23/2023 09:34:53 AM [INFO]:   f1 = 0.7240841777084956\n",
            "05/23/2023 09:34:53 AM [INFO]:   precision = 0.7151655119322555\n",
            "05/23/2023 09:34:53 AM [INFO]:   recall = 0.7332280978689818\n",
            "Epoch finished, took 41.56 seconds.\n",
            "Losses at Epoch 9: 0.4470133\n",
            "Train accuracy at Epoch 9: 0.8606250\n",
            "Test f1 at Epoch 9: 0.7240842\n",
            "[Epoch: 10,   800/ 8000 points] total loss, accuracy per batch: 0.433, 0.860\n",
            "[Epoch: 10,  1600/ 8000 points] total loss, accuracy per batch: 0.443, 0.861\n",
            "[Epoch: 10,  2400/ 8000 points] total loss, accuracy per batch: 0.421, 0.864\n",
            "[Epoch: 10,  3200/ 8000 points] total loss, accuracy per batch: 0.398, 0.891\n",
            "[Epoch: 10,  4000/ 8000 points] total loss, accuracy per batch: 0.416, 0.863\n",
            "[Epoch: 10,  4800/ 8000 points] total loss, accuracy per batch: 0.410, 0.864\n",
            "[Epoch: 10,  5600/ 8000 points] total loss, accuracy per batch: 0.427, 0.855\n",
            "[Epoch: 10,  6400/ 8000 points] total loss, accuracy per batch: 0.508, 0.841\n",
            "[Epoch: 10,  7200/ 8000 points] total loss, accuracy per batch: 0.463, 0.850\n",
            "[Epoch: 10,  8000/ 8000 points] total loss, accuracy per batch: 0.455, 0.863\n",
            "05/23/2023 09:35:46 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  8.95it/s]\n",
            "05/23/2023 09:35:55 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:35:55 AM [INFO]:   accuracy = 0.7991506085192699\n",
            "05/23/2023 09:35:55 AM [INFO]:   f1 = 0.714175654853621\n",
            "05/23/2023 09:35:55 AM [INFO]:   precision = 0.6949025487256372\n",
            "05/23/2023 09:35:55 AM [INFO]:   recall = 0.7345483359746434\n",
            "Epoch finished, took 41.60 seconds.\n",
            "Losses at Epoch 10: 0.4373332\n",
            "Train accuracy at Epoch 10: 0.8611250\n",
            "Test f1 at Epoch 10: 0.7141757\n",
            "[Epoch: 11,   800/ 8000 points] total loss, accuracy per batch: 0.357, 0.892\n",
            "[Epoch: 11,  1600/ 8000 points] total loss, accuracy per batch: 0.403, 0.865\n",
            "[Epoch: 11,  2400/ 8000 points] total loss, accuracy per batch: 0.404, 0.868\n",
            "[Epoch: 11,  3200/ 8000 points] total loss, accuracy per batch: 0.418, 0.880\n",
            "[Epoch: 11,  4000/ 8000 points] total loss, accuracy per batch: 0.426, 0.864\n",
            "[Epoch: 11,  4800/ 8000 points] total loss, accuracy per batch: 0.377, 0.885\n",
            "[Epoch: 11,  5600/ 8000 points] total loss, accuracy per batch: 0.402, 0.863\n",
            "[Epoch: 11,  6400/ 8000 points] total loss, accuracy per batch: 0.450, 0.864\n",
            "[Epoch: 11,  7200/ 8000 points] total loss, accuracy per batch: 0.412, 0.876\n",
            "[Epoch: 11,  8000/ 8000 points] total loss, accuracy per batch: 0.441, 0.854\n",
            "05/23/2023 09:36:56 AM [INFO]: Evaluating test samples...\n",
            "100% 85/85 [00:09<00:00,  9.19it/s]\n",
            "05/23/2023 09:37:06 AM [INFO]: ***** Eval results *****\n",
            "05/23/2023 09:37:06 AM [INFO]:   accuracy = 0.7968686612576066\n",
            "05/23/2023 09:37:06 AM [INFO]:   f1 = 0.7316686484344034\n",
            "05/23/2023 09:37:06 AM [INFO]:   precision = 0.7244897959183674\n",
            "05/23/2023 09:37:06 AM [INFO]:   recall = 0.7389911929543634\n",
            "Epoch finished, took 40.96 seconds.\n",
            "Losses at Epoch 11: 0.4092301\n",
            "Train accuracy at Epoch 11: 0.8710000\n",
            "Test f1 at Epoch 11: 0.7316686\n",
            "05/23/2023 09:37:35 AM [INFO]: Finished Training!\n",
            "05/23/2023 09:37:41 AM [INFO]: Loading tokenizer and model...\n",
            "05/23/2023 09:37:42 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:37:42 AM [INFO]: Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:37:43 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Model config:  {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:37:47 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n",
            "05/23/2023 09:37:49 AM [INFO]: Loaded checkpoint model.\n",
            "05/23/2023 09:37:49 AM [INFO]: Loaded model and optimizer.\n",
            "05/23/2023 09:37:49 AM [INFO]: Done!\n",
            "Sentence:  The surprise [E1]visit[/E1] caused a [E2]frenzy[/E2] on the already chaotic trading floor.\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  [E2]After eating the chicken[/E2] , [E1]he[/E1] developed a sore throat the next morning .\n",
            "Predicted:  Other \n",
            "\n",
            "Sentence:  After eating the chicken , [E1]he[/E1] developed [E2]a sore throat[/E2] the next morning .\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  [E1]After eating the chicken[/E1] , [E2]he[/E2] developed a sore throat the next morning .\n",
            "Predicted:  Product-Producer(e1,e2) \n",
            "\n",
            "Sentence:  [E1]After eating the chicken[/E1] , he developed [E2]a sore throat[/E2] the next morning .\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  After eating the chicken , [E2]he[/E2] developed [E1]a sore throat[/E1] the next morning .\n",
            "Predicted:  Cause-Effect(e2,e1) \n",
            "\n",
            "Sentence:  [E2]After eating the chicken[/E2] , he developed [E1]a sore throat[/E1] the next morning .\n",
            "Predicted:  Cause-Effect(e2,e1) \n",
            "\n",
            "Type input sentence ('quit' or 'exit' to terminate):\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python main_task.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si on veut juste faire une inférence:"
      ],
      "metadata": {
        "id": "7ykIAKrA6DpL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j0jMNP0cKyT",
        "outputId": "7457d51e-3156-43fe-e9af-4ee03c4e4fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-23 09:58:23.980638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/23/2023 09:58:32 AM [INFO]: TensorFlow version 2.12.0 available.\n",
            "05/23/2023 09:58:32 AM [INFO]: PyTorch version 2.0.1+cu118 available.\n",
            "05/23/2023 09:58:32 AM [INFO]: Loaded tokenizer from pre-trained blanks model\n",
            "05/23/2023 09:58:32 AM [INFO]: Loaded preproccessed data.\n",
            "05/23/2023 09:58:32 AM [INFO]: Tokenizing data...\n",
            "prog-bar: 100% 8000/8000 [00:05<00:00, 1409.21it/s]\n",
            "prog-bar: 100% 8000/8000 [00:00<00:00, 112788.76it/s]\n",
            "\n",
            "Invalid rows/total: 0/8000\n",
            "05/23/2023 09:58:37 AM [INFO]: Tokenizing data...\n",
            "prog-bar: 100% 2717/2717 [00:01<00:00, 2066.11it/s]\n",
            "prog-bar: 100% 2717/2717 [00:00<00:00, 105959.31it/s]\n",
            "\n",
            "Invalid rows/total: 0/2717\n",
            "05/23/2023 09:58:39 AM [INFO]: Loaded 8000 Training samples.\n",
            "05/23/2023 09:58:40 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:58:40 AM [INFO]: Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:58:41 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Model config:  {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:58:44 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n",
            "05/23/2023 09:58:45 AM [INFO]: FREEZING MOST HIDDEN LAYERS...\n",
            "[FROZE]: embeddings.word_embeddings.weight\n",
            "[FROZE]: embeddings.position_embeddings.weight\n",
            "[FROZE]: embeddings.token_type_embeddings.weight\n",
            "[FROZE]: embeddings.LayerNorm.weight\n",
            "[FROZE]: embeddings.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.0.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.0.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.0.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.0.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.0.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.0.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.0.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.0.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.0.output.dense.weight\n",
            "[FROZE]: encoder.layer.0.output.dense.bias\n",
            "[FROZE]: encoder.layer.0.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.0.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.1.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.1.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.1.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.1.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.1.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.1.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.1.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.1.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.1.output.dense.weight\n",
            "[FROZE]: encoder.layer.1.output.dense.bias\n",
            "[FROZE]: encoder.layer.1.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.1.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.2.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.2.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.2.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.2.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.2.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.2.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.2.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.2.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.2.output.dense.weight\n",
            "[FROZE]: encoder.layer.2.output.dense.bias\n",
            "[FROZE]: encoder.layer.2.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.2.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.3.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.3.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.3.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.3.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.3.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.3.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.3.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.3.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.3.output.dense.weight\n",
            "[FROZE]: encoder.layer.3.output.dense.bias\n",
            "[FROZE]: encoder.layer.3.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.3.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.4.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.4.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.4.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.4.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.4.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.4.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.4.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.4.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.4.output.dense.weight\n",
            "[FROZE]: encoder.layer.4.output.dense.bias\n",
            "[FROZE]: encoder.layer.4.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.4.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.5.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.5.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.5.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.5.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.5.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.5.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.5.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.5.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.5.output.dense.weight\n",
            "[FROZE]: encoder.layer.5.output.dense.bias\n",
            "[FROZE]: encoder.layer.5.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.5.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.6.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.6.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.6.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.6.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.6.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.6.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.6.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.6.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.6.output.dense.weight\n",
            "[FROZE]: encoder.layer.6.output.dense.bias\n",
            "[FROZE]: encoder.layer.6.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.6.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.7.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.7.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.7.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.7.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.7.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.7.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.7.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.7.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.7.output.dense.weight\n",
            "[FROZE]: encoder.layer.7.output.dense.bias\n",
            "[FROZE]: encoder.layer.7.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.7.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.8.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.8.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.8.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.8.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.8.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.8.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.8.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.8.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.8.output.dense.weight\n",
            "[FROZE]: encoder.layer.8.output.dense.bias\n",
            "[FROZE]: encoder.layer.8.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.8.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.9.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.9.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.9.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.9.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.9.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.9.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.9.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.9.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.9.output.dense.weight\n",
            "[FROZE]: encoder.layer.9.output.dense.bias\n",
            "[FROZE]: encoder.layer.9.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.9.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.query.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.query.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.key.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.key.bias\n",
            "[FROZE]: encoder.layer.10.attention.self.value.weight\n",
            "[FROZE]: encoder.layer.10.attention.self.value.bias\n",
            "[FROZE]: encoder.layer.10.attention.output.dense.weight\n",
            "[FROZE]: encoder.layer.10.attention.output.dense.bias\n",
            "[FROZE]: encoder.layer.10.attention.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.10.attention.output.LayerNorm.bias\n",
            "[FROZE]: encoder.layer.10.intermediate.dense.weight\n",
            "[FROZE]: encoder.layer.10.intermediate.dense.bias\n",
            "[FROZE]: encoder.layer.10.output.dense.weight\n",
            "[FROZE]: encoder.layer.10.output.dense.bias\n",
            "[FROZE]: encoder.layer.10.output.LayerNorm.weight\n",
            "[FROZE]: encoder.layer.10.output.LayerNorm.bias\n",
            "[FREE]: encoder.layer.11.attention.self.query.weight\n",
            "[FREE]: encoder.layer.11.attention.self.query.bias\n",
            "[FREE]: encoder.layer.11.attention.self.key.weight\n",
            "[FREE]: encoder.layer.11.attention.self.key.bias\n",
            "[FREE]: encoder.layer.11.attention.self.value.weight\n",
            "[FREE]: encoder.layer.11.attention.self.value.bias\n",
            "[FREE]: encoder.layer.11.attention.output.dense.weight\n",
            "[FREE]: encoder.layer.11.attention.output.dense.bias\n",
            "[FREE]: encoder.layer.11.attention.output.LayerNorm.weight\n",
            "[FREE]: encoder.layer.11.attention.output.LayerNorm.bias\n",
            "[FREE]: encoder.layer.11.intermediate.dense.weight\n",
            "[FREE]: encoder.layer.11.intermediate.dense.bias\n",
            "[FREE]: encoder.layer.11.output.dense.weight\n",
            "[FREE]: encoder.layer.11.output.dense.bias\n",
            "[FREE]: encoder.layer.11.output.LayerNorm.weight\n",
            "[FREE]: encoder.layer.11.output.LayerNorm.bias\n",
            "[FREE]: pooler.dense.weight\n",
            "[FREE]: pooler.dense.bias\n",
            "[FREE]: classification_layer.weight\n",
            "[FREE]: classification_layer.bias\n",
            "05/23/2023 09:58:45 AM [INFO]: Loaded checkpoint model.\n",
            "05/23/2023 09:58:45 AM [INFO]: Loaded model and optimizer.\n",
            "05/23/2023 09:58:45 AM [INFO]: Loaded results buffer\n",
            "05/23/2023 09:58:45 AM [INFO]: Starting training process...\n",
            "05/23/2023 09:58:45 AM [INFO]: Finished Training!\n",
            "05/23/2023 09:58:52 AM [INFO]: Loading tokenizer and model...\n",
            "05/23/2023 09:58:53 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/23/2023 09:58:53 AM [INFO]: Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:58:54 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Model config:  {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/23/2023 09:58:57 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n",
            "05/23/2023 09:59:06 AM [INFO]: Loaded checkpoint model.\n",
            "05/23/2023 09:59:06 AM [INFO]: Loaded model and optimizer.\n",
            "05/23/2023 09:59:06 AM [INFO]: Done!\n",
            "Sentence:  The surprise [E1]visit[/E1] caused a [E2]frenzy[/E2] on the already chaotic trading floor.\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  [E2]After eating the chicken[/E2] , [E1]he[/E1] developed a sore throat the next morning .\n",
            "Predicted:  Other \n",
            "\n",
            "Sentence:  After eating the chicken , [E1]he[/E1] developed [E2]a sore throat[/E2] the next morning .\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  [E1]After eating the chicken[/E1] , [E2]he[/E2] developed a sore throat the next morning .\n",
            "Predicted:  Product-Producer(e1,e2) \n",
            "\n",
            "Sentence:  [E1]After eating the chicken[/E1] , he developed [E2]a sore throat[/E2] the next morning .\n",
            "Predicted:  Cause-Effect(e1,e2) \n",
            "\n",
            "Sentence:  After eating the chicken , [E2]he[/E2] developed [E1]a sore throat[/E1] the next morning .\n",
            "Predicted:  Cause-Effect(e2,e1) \n",
            "\n",
            "Sentence:  [E2]After eating the chicken[/E2] , he developed [E1]a sore throat[/E1] the next morning .\n",
            "Predicted:  Cause-Effect(e2,e1) \n",
            "\n",
            "Type input sentence ('quit' or 'exit' to terminate):\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python main_task.py --infer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmNrLgJ0AfWF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}